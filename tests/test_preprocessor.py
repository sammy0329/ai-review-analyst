"""
ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ë‹¨ìœ„ í…ŒìŠ¤íŠ¸.
"""

import sys
from dataclasses import dataclass, field
from typing import Any
from unittest.mock import MagicMock

import pytest

# playwright ì˜ì¡´ì„± ì—†ì´ í…ŒìŠ¤íŠ¸í•  ìˆ˜ ìˆë„ë¡ ëª¨í‚¹
sys.modules["playwright"] = MagicMock()
sys.modules["playwright.async_api"] = MagicMock()
sys.modules["playwright_stealth"] = MagicMock()

from src.crawler.base import Review
from src.pipeline.preprocessor import (
    DuplicateFilter,
    ProcessedReview,
    ReviewPreprocessor,
    TextChunk,
    TextChunker,
    TextCleaner,
    create_default_preprocessor,
)


class TestTextCleaner:
    """TextCleaner í…ŒìŠ¤íŠ¸."""

    def test_basic_cleaning(self):
        """ê¸°ë³¸ í…ìŠ¤íŠ¸ ì •ì œ í…ŒìŠ¤íŠ¸."""
        cleaner = TextCleaner()
        text = "ì´ ì œí’ˆ   ì •ë§    ì¢‹ì•„ìš”!"
        result = cleaner.clean(text)
        assert "  " not in result  # ì—°ì† ê³µë°± ì œê±° í™•ì¸

    def test_emoji_removal(self):
        """ì´ëª¨ì§€ ì œê±° í…ŒìŠ¤íŠ¸."""
        cleaner = TextCleaner(remove_emojis=True)
        text = "ì¢‹ì•„ìš”! ğŸ‘ğŸ‘ ìµœê³ ì˜ˆìš”! ğŸ‰"
        result = cleaner.clean(text)
        assert "ğŸ‘" not in result
        assert "ğŸ‰" not in result
        assert "ì¢‹ì•„ìš”" in result

    def test_emoji_preservation(self):
        """ì´ëª¨ì§€ ë³´ì¡´ í…ŒìŠ¤íŠ¸."""
        cleaner = TextCleaner(remove_emojis=False)
        text = "ì¢‹ì•„ìš”! ğŸ‘"
        result = cleaner.clean(text)
        assert "ğŸ‘" in result

    def test_repeated_char_normalization(self):
        """ë°˜ë³µ ë¬¸ì ì •ê·œí™” í…ŒìŠ¤íŠ¸."""
        cleaner = TextCleaner(normalize_repeated_chars=True, max_repeated_chars=2)
        text = "ì •ë§ ì¢‹ì•„ìš”ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹"
        result = cleaner.clean(text)
        assert "ã…‹ã…‹ã…‹" not in result
        assert "ã…‹ã…‹" in result

    def test_special_char_removal(self):
        """íŠ¹ìˆ˜ë¬¸ì ì œê±° í…ŒìŠ¤íŠ¸."""
        # remove_emojis=Trueë¡œ ì„¤ì •í•´ì•¼ ì´ëª¨ì§€ ìœ ë‹ˆì½”ë“œ ë²”ìœ„ì˜ ê¸°í˜¸(â˜…, â™¡)ë„ ì œê±°ë¨
        cleaner = TextCleaner(remove_special_chars=True, remove_emojis=True)
        text = "ì œí’ˆâ˜…â˜…â˜… ì •ë§ ì¢‹ì•„ìš”â™¡â™¡"
        result = cleaner.clean(text)
        assert "â˜…" not in result
        assert "â™¡" not in result
        assert "ì œí’ˆ" in result
        assert "ì¢‹ì•„ìš”" in result

    def test_html_entity_removal(self):
        """HTML ì—”í‹°í‹° ì œê±° í…ŒìŠ¤íŠ¸."""
        cleaner = TextCleaner(remove_html_entities=True)
        text = "ê°€ê²©ì´ &lt;10000ì›&gt; ì…ë‹ˆë‹¤&nbsp;ì¢‹ì•„ìš”"
        result = cleaner.clean(text)
        assert "&lt;" not in result
        assert "&gt;" not in result
        assert "&nbsp;" not in result

    def test_whitespace_normalization(self):
        """ê³µë°± ì •ê·œí™” í…ŒìŠ¤íŠ¸."""
        cleaner = TextCleaner(normalize_whitespace=True)
        text = "ì²« ë²ˆì§¸ ì¤„\n\n\në‘ ë²ˆì§¸ ì¤„   ì„¸ ë²ˆì§¸"
        result = cleaner.clean(text)
        # ì—°ì† ë¹ˆ ì¤„ê³¼ ê³µë°±ì´ ì •ê·œí™”ë˜ì–´ì•¼ í•¨
        assert "\n\n\n" not in result
        assert "   " not in result

    def test_lowercase_conversion(self):
        """ì†Œë¬¸ì ë³€í™˜ í…ŒìŠ¤íŠ¸."""
        cleaner = TextCleaner(lowercase=True)
        text = "HELLO World ì•ˆë…•í•˜ì„¸ìš”"
        result = cleaner.clean(text)
        assert "hello" in result
        assert "world" in result
        assert "ì•ˆë…•í•˜ì„¸ìš”" in result  # í•œê¸€ì€ ì˜í–¥ ì—†ìŒ

    def test_empty_text(self):
        """ë¹ˆ í…ìŠ¤íŠ¸ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸."""
        cleaner = TextCleaner()
        assert cleaner.clean("") == ""
        assert cleaner.clean(None) == ""

    def test_unicode_normalization(self):
        """Unicode ì •ê·œí™” í…ŒìŠ¤íŠ¸ (í•œê¸€ ìëª¨ ê²°í•©)."""
        cleaner = TextCleaner()
        # ë¶„ë¦¬ëœ ìëª¨ (ã…ã…ã„´ã„±ã…œã„¹) â†’ ê²°í•©ëœ í•œê¸€ (í•œê¸€)
        decomposed = "\u1112\u1161\u11ab\u1100\u116e\u11af"  # í•œê¸€ (ë¶„ë¦¬í˜•)
        result = cleaner.clean(decomposed)
        assert result  # ì •ê·œí™” í›„ í…ìŠ¤íŠ¸ê°€ ìˆì–´ì•¼ í•¨


class TestTextChunker:
    """TextChunker í…ŒìŠ¤íŠ¸."""

    def test_short_text_single_chunk(self):
        """ì§§ì€ í…ìŠ¤íŠ¸ ë‹¨ì¼ ì²­í¬ í…ŒìŠ¤íŠ¸."""
        chunker = TextChunker(chunk_size=500)
        text = "ì§§ì€ ë¦¬ë·°ì…ë‹ˆë‹¤."
        chunks = chunker.chunk(text)
        assert len(chunks) == 1
        assert chunks[0].text == text

    def test_long_text_multiple_chunks(self):
        """ê¸´ í…ìŠ¤íŠ¸ ë‹¤ì¤‘ ì²­í¬ í…ŒìŠ¤íŠ¸."""
        chunker = TextChunker(chunk_size=50, min_chunk_size=10, chunk_overlap=10)
        text = "ì´ê²ƒì€ ì²« ë²ˆì§¸ ë¬¸ì¥ì…ë‹ˆë‹¤. ì´ê²ƒì€ ë‘ ë²ˆì§¸ ë¬¸ì¥ì…ë‹ˆë‹¤. ì´ê²ƒì€ ì„¸ ë²ˆì§¸ ë¬¸ì¥ì…ë‹ˆë‹¤. ì´ê²ƒì€ ë„¤ ë²ˆì§¸ ë¬¸ì¥ì…ë‹ˆë‹¤."
        chunks = chunker.chunk(text)
        assert len(chunks) > 1

    def test_chunk_overlap(self):
        """ì²­í¬ ì˜¤ë²„ë© í…ŒìŠ¤íŠ¸."""
        chunker = TextChunker(chunk_size=100, chunk_overlap=20, min_chunk_size=20)
        text = "A" * 50 + ". " + "B" * 50 + ". " + "C" * 50 + "."
        chunks = chunker.chunk(text)

        # ì²­í¬ê°€ ì—¬ëŸ¬ ê°œ ìƒì„±ë˜ì–´ì•¼ í•¨
        assert len(chunks) >= 2

    def test_sentence_boundary_split(self):
        """ë¬¸ì¥ ê²½ê³„ ë¶„í•  í…ŒìŠ¤íŠ¸."""
        chunker = TextChunker(chunk_size=100, split_by_sentence=True, min_chunk_size=10)
        text = "ì²« ë²ˆì§¸ ë¬¸ì¥ì…ë‹ˆë‹¤. ë‘ ë²ˆì§¸ ë¬¸ì¥ì…ë‹ˆë‹¤! ì„¸ ë²ˆì§¸ ë¬¸ì¥ì…ë‹ˆë‹¤?"
        chunks = chunker.chunk(text)

        # ê° ì²­í¬ê°€ ì™„ì „í•œ ë¬¸ì¥ì„ í¬í•¨í•´ì•¼ í•¨
        for chunk in chunks:
            # ì²­í¬ê°€ ë¬¸ì¥ ì¤‘ê°„ì—ì„œ ì˜ë¦¬ì§€ ì•Šì•˜ëŠ”ì§€ í™•ì¸
            assert chunk.text.strip()

    def test_chunk_indices(self):
        """ì²­í¬ ì¸ë±ìŠ¤ í…ŒìŠ¤íŠ¸."""
        chunker = TextChunker(chunk_size=30, min_chunk_size=10)
        text = "ì²« ë²ˆì§¸. ë‘ ë²ˆì§¸. ì„¸ ë²ˆì§¸. ë„¤ ë²ˆì§¸."
        chunks = chunker.chunk(text)

        for i, chunk in enumerate(chunks):
            assert chunk.chunk_index == i

    def test_empty_text(self):
        """ë¹ˆ í…ìŠ¤íŠ¸ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸."""
        chunker = TextChunker()
        assert chunker.chunk("") == []
        assert chunker.chunk(None) == []


class TestDuplicateFilter:
    """DuplicateFilter í…ŒìŠ¤íŠ¸."""

    def test_exact_duplicate_detection(self):
        """ì •í™•í•œ ì¤‘ë³µ ê°ì§€ í…ŒìŠ¤íŠ¸."""
        filter = DuplicateFilter()

        filter.add("ì´ ì œí’ˆ ì •ë§ ì¢‹ì•„ìš”")
        assert filter.is_duplicate("ì´ ì œí’ˆ ì •ë§ ì¢‹ì•„ìš”") is True
        assert filter.is_duplicate("ë‹¤ë¥¸ ì œí’ˆ ë¦¬ë·°") is False

    def test_whitespace_normalized_duplicate(self):
        """ê³µë°± ì •ê·œí™” ì¤‘ë³µ ê°ì§€ í…ŒìŠ¤íŠ¸."""
        filter = DuplicateFilter()

        filter.add("ì´ ì œí’ˆ ì •ë§ ì¢‹ì•„ìš”")
        # ê³µë°±ë§Œ ë‹¤ë¥¸ ê²½ìš°ë„ ì¤‘ë³µìœ¼ë¡œ ì²˜ë¦¬
        assert filter.is_duplicate("ì´ ì œí’ˆ  ì •ë§   ì¢‹ì•„ìš”") is True

    def test_fuzzy_duplicate_detection(self):
        """í¼ì§€ ì¤‘ë³µ ê°ì§€ í…ŒìŠ¤íŠ¸."""
        filter = DuplicateFilter(use_fuzzy=True, fuzzy_threshold=0.7)

        filter.add("ì´ ì œí’ˆ ì •ë§ ì¢‹ì•„ìš” ë°°ì†¡ë„ ë¹¨ë¼ìš”")
        # ìœ ì‚¬í•œ í…ìŠ¤íŠ¸
        assert filter.is_duplicate("ì´ ì œí’ˆ ì •ë§ ì¢‹ì•„ìš” ë°°ì†¡ ë¹¨ë¼ìš”") is True
        # ë‹¤ë¥¸ í…ìŠ¤íŠ¸
        assert filter.is_duplicate("ì™„ì „íˆ ë‹¤ë¥¸ ë‚´ìš©ì˜ ë¦¬ë·°ì…ë‹ˆë‹¤") is False

    def test_filter_list(self):
        """ë¦¬ìŠ¤íŠ¸ í•„í„°ë§ í…ŒìŠ¤íŠ¸."""
        filter = DuplicateFilter()

        texts = [
            "ì²« ë²ˆì§¸ ë¦¬ë·°",
            "ë‘ ë²ˆì§¸ ë¦¬ë·°",
            "ì²« ë²ˆì§¸ ë¦¬ë·°",  # ì¤‘ë³µ
            "ì„¸ ë²ˆì§¸ ë¦¬ë·°",
            "ë‘ ë²ˆì§¸ ë¦¬ë·°",  # ì¤‘ë³µ
        ]

        filtered = filter.filter(texts)
        assert len(filtered) == 3
        assert "ì²« ë²ˆì§¸ ë¦¬ë·°" in filtered
        assert "ë‘ ë²ˆì§¸ ë¦¬ë·°" in filtered
        assert "ì„¸ ë²ˆì§¸ ë¦¬ë·°" in filtered

    def test_reset(self):
        """í•„í„° ë¦¬ì…‹ í…ŒìŠ¤íŠ¸."""
        filter = DuplicateFilter()

        filter.add("í…ŒìŠ¤íŠ¸ í…ìŠ¤íŠ¸")
        assert filter.is_duplicate("í…ŒìŠ¤íŠ¸ í…ìŠ¤íŠ¸") is True

        filter.reset()
        assert filter.is_duplicate("í…ŒìŠ¤íŠ¸ í…ìŠ¤íŠ¸") is False

    def test_hash_computation(self):
        """í•´ì‹œ ê³„ì‚° í…ŒìŠ¤íŠ¸."""
        hash1 = DuplicateFilter._compute_hash("í…ŒìŠ¤íŠ¸ í…ìŠ¤íŠ¸")
        hash2 = DuplicateFilter._compute_hash("í…ŒìŠ¤íŠ¸ í…ìŠ¤íŠ¸")
        hash3 = DuplicateFilter._compute_hash("ë‹¤ë¥¸ í…ìŠ¤íŠ¸")

        assert hash1 == hash2  # ë™ì¼ í…ìŠ¤íŠ¸ëŠ” ë™ì¼ í•´ì‹œ
        assert hash1 != hash3  # ë‹¤ë¥¸ í…ìŠ¤íŠ¸ëŠ” ë‹¤ë¥¸ í•´ì‹œ

    def test_similarity_calculation(self):
        """ìœ ì‚¬ë„ ê³„ì‚° í…ŒìŠ¤íŠ¸."""
        # ë™ì¼ í…ìŠ¤íŠ¸
        sim1 = DuplicateFilter._similarity("a b c d e", "a b c d e")
        assert sim1 == 1.0

        # ì™„ì „íˆ ë‹¤ë¥¸ í…ìŠ¤íŠ¸
        sim2 = DuplicateFilter._similarity("a b c", "x y z")
        assert sim2 == 0.0

        # ë¶€ë¶„ ìœ ì‚¬ í…ìŠ¤íŠ¸
        sim3 = DuplicateFilter._similarity("a b c d", "a b c e")
        assert 0 < sim3 < 1


class TestReviewPreprocessor:
    """ReviewPreprocessor í…ŒìŠ¤íŠ¸."""

    @pytest.fixture
    def sample_review(self):
        """ìƒ˜í”Œ ë¦¬ë·° í”½ìŠ¤ì²˜."""
        return Review(
            text="ì´ ì œí’ˆ ì •ë§ ì¢‹ì•„ìš”ã…‹ã…‹ã…‹ã…‹ ë°°ì†¡ë„ ë¹¨ë¼ì„œ ë§¤ìš° ë§Œì¡±í•©ë‹ˆë‹¤!!! í’ˆì§ˆë„ ê°€ê²© ëŒ€ë¹„ í›Œë¥­í•˜ë„¤ìš”. ğŸ‘",
            rating=5.0,
            date="2024-01-15",
            author="í…ŒìŠ¤í„°",
            option="ë¸”ë™ / L",
            verified_purchase=True,
        )

    @pytest.fixture
    def preprocessor(self):
        """ì „ì²˜ë¦¬ê¸° í”½ìŠ¤ì²˜."""
        return create_default_preprocessor(
            chunk_size=500,
            remove_emojis=True,
        )

    def test_basic_processing(self, preprocessor, sample_review):
        """ê¸°ë³¸ ì „ì²˜ë¦¬ í…ŒìŠ¤íŠ¸."""
        result = preprocessor.process(sample_review)

        assert result is not None
        assert isinstance(result, ProcessedReview)
        assert result.original_text == sample_review.text
        assert result.cleaned_text != result.original_text  # ì •ì œë¨
        assert len(result.chunks) >= 1
        assert result.text_hash  # í•´ì‹œ ìƒì„±ë¨
        assert result.rating == 5.0

    def test_short_text_filtering(self):
        """ì§§ì€ í…ìŠ¤íŠ¸ í•„í„°ë§ í…ŒìŠ¤íŠ¸."""
        preprocessor = ReviewPreprocessor(min_text_length=20)
        short_review = Review(text="ì¢‹ì•„ìš”", rating=5.0)

        result = preprocessor.process(short_review)
        assert result is None  # í•„í„°ë§ë¨

    def test_long_text_truncation(self):
        """ê¸´ í…ìŠ¤íŠ¸ ì˜ë¼ë‚´ê¸° í…ŒìŠ¤íŠ¸."""
        preprocessor = ReviewPreprocessor(max_text_length=100, min_text_length=5)
        long_review = Review(text="ì¢‹ì€ ì œí’ˆì…ë‹ˆë‹¤ " * 50, rating=5.0)

        result = preprocessor.process(long_review)
        assert result is not None
        assert len(result.cleaned_text) <= 100

    def test_duplicate_filtering(self):
        """ì¤‘ë³µ ë¦¬ë·° í•„í„°ë§ í…ŒìŠ¤íŠ¸."""
        preprocessor = create_default_preprocessor()

        review1 = Review(text="ì´ ì œí’ˆ ì •ë§ ì¢‹ìŠµë‹ˆë‹¤. í’ˆì§ˆì´ í›Œë¥­í•˜ê³  ë°°ì†¡ë„ ë¹ ë¦…ë‹ˆë‹¤. ì¶”ì²œí•´ìš”!", rating=5.0)
        review2 = Review(text="ì´ ì œí’ˆ ì •ë§ ì¢‹ìŠµë‹ˆë‹¤. í’ˆì§ˆì´ í›Œë¥­í•˜ê³  ë°°ì†¡ë„ ë¹ ë¦…ë‹ˆë‹¤. ì¶”ì²œí•´ìš”!", rating=4.0)  # ë™ì¼ í…ìŠ¤íŠ¸

        result1 = preprocessor.process(review1)
        result2 = preprocessor.process(review2)

        assert result1 is not None
        assert result2 is None  # ì¤‘ë³µìœ¼ë¡œ í•„í„°ë§

    def test_skip_duplicate_check(self):
        """ì¤‘ë³µ ê²€ì‚¬ ìŠ¤í‚µ í…ŒìŠ¤íŠ¸."""
        preprocessor = create_default_preprocessor()

        review1 = Review(text="ì´ ì œí’ˆ ì •ë§ ì¢‹ìŠµë‹ˆë‹¤. í’ˆì§ˆì´ í›Œë¥­í•˜ê³  ë°°ì†¡ë„ ë¹ ë¦…ë‹ˆë‹¤. ì¶”ì²œí•´ìš”!", rating=5.0)
        review2 = Review(text="ì´ ì œí’ˆ ì •ë§ ì¢‹ìŠµë‹ˆë‹¤. í’ˆì§ˆì´ í›Œë¥­í•˜ê³  ë°°ì†¡ë„ ë¹ ë¦…ë‹ˆë‹¤. ì¶”ì²œí•´ìš”!", rating=4.0)

        result1 = preprocessor.process(review1, skip_duplicate_check=True)
        result2 = preprocessor.process(review2, skip_duplicate_check=True)

        assert result1 is not None
        assert result2 is not None  # ìŠ¤í‚µìœ¼ë¡œ ì¸í•´ í†µê³¼

    def test_batch_processing(self, preprocessor):
        """ë°°ì¹˜ ì „ì²˜ë¦¬ í…ŒìŠ¤íŠ¸."""
        reviews = [
            Review(text="ì²« ë²ˆì§¸ ë¦¬ë·°ì…ë‹ˆë‹¤. ì œí’ˆì´ ì •ë§ ì¢‹ì•„ìš”. í’ˆì§ˆì´ í›Œë¥­í•©ë‹ˆë‹¤.", rating=5.0),
            Review(text="ë‘ ë²ˆì§¸ ë¦¬ë·°ì…ë‹ˆë‹¤. ë°°ì†¡ì´ ë¹¨ë¼ìš”. í¬ì¥ë„ ê¼¼ê¼¼í•©ë‹ˆë‹¤.", rating=4.0),
            Review(text="ì„¸ ë²ˆì§¸ ë¦¬ë·°ì…ë‹ˆë‹¤. ê°€ê²©ì´ ì°©í•´ìš”. ê°€ì„±ë¹„ ìµœê³ ì…ë‹ˆë‹¤.", rating=4.5),
        ]

        results = preprocessor.process_batch(reviews)
        assert len(results) == 3

    def test_metadata_extraction(self, preprocessor, sample_review):
        """ë©”íƒ€ë°ì´í„° ì¶”ì¶œ í…ŒìŠ¤íŠ¸."""
        result = preprocessor.process(sample_review)

        assert result is not None
        assert result.metadata["author"] == "í…ŒìŠ¤í„°"
        assert result.metadata["option"] == "ë¸”ë™ / L"
        assert result.metadata["verified_purchase"] is True
        assert "text_length" in result.metadata
        assert "word_count" in result.metadata

    def test_statistics(self, preprocessor):
        """í†µê³„ ê³„ì‚° í…ŒìŠ¤íŠ¸."""
        reviews = [
            Review(text="ì²« ë²ˆì§¸ ë¦¬ë·°ì…ë‹ˆë‹¤. ì œí’ˆì´ ì •ë§ ì¢‹ì•„ìš”. í’ˆì§ˆì´ í›Œë¥­í•©ë‹ˆë‹¤.", rating=5.0),
            Review(text="ë‘ ë²ˆì§¸ ë¦¬ë·°ì…ë‹ˆë‹¤. ë°°ì†¡ì´ ë¹¨ë¼ìš”. í¬ì¥ë„ ê¼¼ê¼¼í•©ë‹ˆë‹¤.", rating=4.0),
            Review(text="ì„¸ ë²ˆì§¸ ë¦¬ë·°ì…ë‹ˆë‹¤. ê°€ê²©ì´ ì°©í•´ìš”. ê°€ì„±ë¹„ ìµœê³ ì…ë‹ˆë‹¤.", rating=3.0),
        ]

        processed = preprocessor.process_batch(reviews)
        stats = preprocessor.get_statistics(processed)

        assert stats["total_reviews"] == 3
        assert stats["total_chunks"] >= 3
        assert stats["avg_rating"] == 4.0
        assert stats["reviews_with_rating"] == 3

    def test_reset(self, preprocessor):
        """íŒŒì´í”„ë¼ì¸ ë¦¬ì…‹ í…ŒìŠ¤íŠ¸."""
        review = Review(text="í…ŒìŠ¤íŠ¸ ë¦¬ë·°ì…ë‹ˆë‹¤. ì´ ì œí’ˆ ì •ë§ ì¢‹ì•„ìš”. í’ˆì§ˆì´ í›Œë¥­í•©ë‹ˆë‹¤.", rating=5.0)

        result1 = preprocessor.process(review)
        result2 = preprocessor.process(review)  # ì¤‘ë³µ

        assert result1 is not None
        assert result2 is None

        preprocessor.reset()

        result3 = preprocessor.process(review)
        assert result3 is not None  # ë¦¬ì…‹ í›„ ë‹¤ì‹œ ì²˜ë¦¬ ê°€ëŠ¥


class TestCreateDefaultPreprocessor:
    """create_default_preprocessor í…ŒìŠ¤íŠ¸."""

    def test_default_creation(self):
        """ê¸°ë³¸ ì „ì²˜ë¦¬ê¸° ìƒì„± í…ŒìŠ¤íŠ¸."""
        preprocessor = create_default_preprocessor()
        assert isinstance(preprocessor, ReviewPreprocessor)
        assert isinstance(preprocessor.cleaner, TextCleaner)
        assert isinstance(preprocessor.chunker, TextChunker)
        assert isinstance(preprocessor.duplicate_filter, DuplicateFilter)

    def test_custom_chunk_size(self):
        """ì»¤ìŠ¤í…€ ì²­í¬ í¬ê¸° í…ŒìŠ¤íŠ¸."""
        preprocessor = create_default_preprocessor(chunk_size=100, chunk_overlap=20)
        assert preprocessor.chunker.chunk_size == 100
        assert preprocessor.chunker.chunk_overlap == 20

    def test_emoji_removal_option(self):
        """ì´ëª¨ì§€ ì œê±° ì˜µì…˜ í…ŒìŠ¤íŠ¸."""
        preprocessor = create_default_preprocessor(remove_emojis=True)
        assert preprocessor.cleaner.remove_emojis is True

    def test_fuzzy_dedup_option(self):
        """í¼ì§€ ì¤‘ë³µ ì œê±° ì˜µì…˜ í…ŒìŠ¤íŠ¸."""
        preprocessor = create_default_preprocessor(use_fuzzy_dedup=True)
        assert preprocessor.duplicate_filter.use_fuzzy is True


class TestTextChunkDataclass:
    """TextChunk ë°ì´í„°í´ë˜ìŠ¤ í…ŒìŠ¤íŠ¸."""

    def test_chunk_creation(self):
        """ì²­í¬ ìƒì„± í…ŒìŠ¤íŠ¸."""
        chunk = TextChunk(
            text="í…ŒìŠ¤íŠ¸ í…ìŠ¤íŠ¸",
            chunk_index=0,
            start_char=0,
            end_char=10,
            metadata={"key": "value"},
        )

        assert chunk.text == "í…ŒìŠ¤íŠ¸ í…ìŠ¤íŠ¸"
        assert chunk.chunk_index == 0
        assert chunk.start_char == 0
        assert chunk.end_char == 10
        assert chunk.metadata["key"] == "value"

    def test_default_metadata(self):
        """ê¸°ë³¸ ë©”íƒ€ë°ì´í„° í…ŒìŠ¤íŠ¸."""
        chunk = TextChunk(text="í…ìŠ¤íŠ¸", chunk_index=0, start_char=0, end_char=5)
        assert chunk.metadata == {}


class TestProcessedReviewDataclass:
    """ProcessedReview ë°ì´í„°í´ë˜ìŠ¤ í…ŒìŠ¤íŠ¸."""

    def test_processed_review_creation(self):
        """ì „ì²˜ë¦¬ëœ ë¦¬ë·° ìƒì„± í…ŒìŠ¤íŠ¸."""
        chunk = TextChunk(text="ì •ì œëœ í…ìŠ¤íŠ¸", chunk_index=0, start_char=0, end_char=10)

        processed = ProcessedReview(
            original_text="ì›ë³¸ í…ìŠ¤íŠ¸",
            cleaned_text="ì •ì œëœ í…ìŠ¤íŠ¸",
            chunks=[chunk],
            text_hash="abc123",
            rating=4.5,
            date="2024-01-15",
            metadata={"source": "test"},
        )

        assert processed.original_text == "ì›ë³¸ í…ìŠ¤íŠ¸"
        assert processed.cleaned_text == "ì •ì œëœ í…ìŠ¤íŠ¸"
        assert len(processed.chunks) == 1
        assert processed.text_hash == "abc123"
        assert processed.rating == 4.5
        assert processed.date == "2024-01-15"
        assert processed.metadata["source"] == "test"


if __name__ == "__main__":
    pytest.main([__file__, "-v"])
