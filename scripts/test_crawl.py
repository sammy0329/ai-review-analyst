"""
í¬ë¡¤ëŸ¬ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸

ì‚¬ìš©ë²•:
    python scripts/test_crawl.py <URL>

ì˜ˆì‹œ:
    python scripts/test_crawl.py "https://www.coupang.com/vp/products/123456"
"""

import asyncio
import json
import sys

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ pathì— ì¶”ê°€
sys.path.insert(0, ".")

from src.crawler import get_crawler, CrawlerError


async def test_crawl(url: str):
    """URLì„ í¬ë¡¤ë§í•˜ê³  ê²°ê³¼ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤."""
    print(f"\n{'='*60}")
    print(f"í¬ë¡¤ë§ í…ŒìŠ¤íŠ¸: {url}")
    print(f"{'='*60}\n")

    try:
        # URLì— ë§ëŠ” í¬ë¡¤ëŸ¬ ê°€ì ¸ì˜¤ê¸°
        crawler = get_crawler(url, headless=True, timeout=30000)
        print(f"ì‚¬ìš© í¬ë¡¤ëŸ¬: {crawler.__class__.__name__}")

        # í¬ë¡¤ë§ ì‹¤í–‰
        print("í¬ë¡¤ë§ ì¤‘...")
        async with crawler:
            result = await crawler.crawl(url, max_pages=3)

        # ê²°ê³¼ ì¶œë ¥
        if result.success:
            print(f"\nâœ… í¬ë¡¤ë§ ì„±ê³µ!")
            print(f"\nğŸ“¦ ìƒí’ˆ ì •ë³´:")
            print(f"   - ìƒí’ˆëª…: {result.product_name or '(ì¶”ì¶œ ì‹¤íŒ¨)'}")
            print(f"   - ê°€ê²©: {result.product_price or '(ì¶”ì¶œ ì‹¤íŒ¨)'}")
            print(f"   - í‰ê·  í‰ì : {result.average_rating or '(ì¶”ì¶œ ì‹¤íŒ¨)'}")
            print(f"   - ìˆ˜ì§‘ëœ ë¦¬ë·° ìˆ˜: {result.total_reviews}")

            if result.reviews:
                print(f"\nğŸ“ ë¦¬ë·° ìƒ˜í”Œ (ìµœëŒ€ 5ê°œ):")
                for i, review in enumerate(result.reviews[:5], 1):
                    print(f"\n   [{i}] â­ {review.rating or '?'}ì ")
                    print(f"       {review.text[:100]}{'...' if len(review.text) > 100 else ''}")
                    if review.date:
                        print(f"       ğŸ“… {review.date}")
                    if review.option:
                        print(f"       ğŸ·ï¸ {review.option}")

            # JSONìœ¼ë¡œ ì €ì¥
            output_file = "data/crawl_result.json"
            import os
            os.makedirs("data", exist_ok=True)
            with open(output_file, "w", encoding="utf-8") as f:
                json.dump(result.to_dict(), f, ensure_ascii=False, indent=2)
            print(f"\nğŸ’¾ ì „ì²´ ê²°ê³¼ ì €ì¥ë¨: {output_file}")

        else:
            print(f"\nâŒ í¬ë¡¤ë§ ì‹¤íŒ¨: {result.error_message}")

    except ValueError as e:
        print(f"\nâŒ ì§€ì›í•˜ì§€ ì•ŠëŠ” URL: {e}")
    except CrawlerError as e:
        print(f"\nâŒ í¬ë¡¤ëŸ¬ ì—ëŸ¬: {e}")
    except Exception as e:
        print(f"\nâŒ ì˜ˆìƒì¹˜ ëª»í•œ ì—ëŸ¬: {e}")
        import traceback
        traceback.print_exc()


def main():
    if len(sys.argv) < 2:
        print(__doc__)
        print("\nì§€ì› í”Œë«í¼:")
        print("  - ì¿ íŒ¡: https://www.coupang.com/vp/products/...")
        print("  - ì˜¬ì›¨ì´ì¦ˆ: https://always.co.kr/products/...")
        sys.exit(1)

    url = sys.argv[1]
    asyncio.run(test_crawl(url))


if __name__ == "__main__":
    main()
