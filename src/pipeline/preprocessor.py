"""
ë¦¬ë·° ë°ì´í„° ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸.

í…ìŠ¤íŠ¸ ì •ì œ, ì²­í‚¹, ë©”íƒ€ë°ì´í„° ì¶”ì¶œ, ì¤‘ë³µ ì œê±° ë“±ì˜ ì „ì²˜ë¦¬ ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.
"""

import hashlib
import re
import unicodedata
from dataclasses import dataclass, field
from typing import Any, Iterator

from src.crawler.base import Review


# ì´ëª¨ì§€ íŒ¨í„´ (Unicode Emoji ranges)
# ì£¼ì˜: ë²”ìœ„ê°€ í•œê¸€ ì˜ì—­(U+AC00-U+D7A3)ê³¼ ê²¹ì¹˜ì§€ ì•Šë„ë¡ ë¶„ë¦¬
EMOJI_PATTERN = re.compile(
    "["
    "\U0001F600-\U0001F64F"  # emoticons
    "\U0001F300-\U0001F5FF"  # symbols & pictographs
    "\U0001F680-\U0001F6FF"  # transport & map symbols
    "\U0001F1E0-\U0001F1FF"  # flags
    "\U00002702-\U000027B0"  # dingbats
    "\U000024C2-\U000024FF"  # enclosed alphanumerics (subset, avoiding Korean range)
    "\U0001F900-\U0001F9FF"  # supplemental symbols
    "\U0001FA00-\U0001FA6F"  # chess symbols
    "\U0001FA70-\U0001FAFF"  # symbols & pictographs extended-A
    "\U00002600-\U000026FF"  # misc symbols
    "\U00002300-\U000023FF"  # misc technical
    "\U0001F200-\U0001F251"  # enclosed ideographic supplement
    "]+",
    flags=re.UNICODE,
)

# íŠ¹ìˆ˜ë¬¸ì íŒ¨í„´ (í•œê¸€, ì˜ë¬¸, ìˆ«ì, ê¸°ë³¸ ë¬¸ì¥ë¶€í˜¸ ì œì™¸)
SPECIAL_CHAR_PATTERN = re.compile(r"[^\w\sê°€-í£ã„±-ã…ã…-ã…£a-zA-Z0-9.,!?~\-()\"':\n]")

# ë°˜ë³µ ë¬¸ì íŒ¨í„´ (ì˜ˆ: ã…‹ã…‹ã…‹ã…‹ã…‹, ã…ã…ã…ã…, !!!!)
REPEATED_CHAR_PATTERN = re.compile(r"(.)\1{3,}")

# ì—°ì† ê³µë°± íŒ¨í„´
MULTIPLE_SPACES_PATTERN = re.compile(r"\s+")

# HTML ì—”í‹°í‹° íŒ¨í„´
HTML_ENTITY_PATTERN = re.compile(r"&[a-zA-Z]+;|&#\d+;")


@dataclass
class TextChunk:
    """í…ìŠ¤íŠ¸ ì²­í¬ ë°ì´í„° êµ¬ì¡°."""

    text: str
    chunk_index: int
    start_char: int
    end_char: int
    metadata: dict[str, Any] = field(default_factory=dict)


@dataclass
class ProcessedReview:
    """ì „ì²˜ë¦¬ëœ ë¦¬ë·° ë°ì´í„° êµ¬ì¡°."""

    original_text: str
    cleaned_text: str
    chunks: list[TextChunk]
    text_hash: str
    rating: float | None
    date: str | None
    metadata: dict[str, Any] = field(default_factory=dict)


class TextCleaner:
    """í…ìŠ¤íŠ¸ ì •ì œ í´ë˜ìŠ¤."""

    def __init__(
        self,
        remove_emojis: bool = False,
        remove_special_chars: bool = True,
        normalize_repeated_chars: bool = True,
        max_repeated_chars: int = 2,
        remove_html_entities: bool = True,
        normalize_whitespace: bool = True,
        lowercase: bool = False,
    ):
        """
        ì´ˆê¸°í™”.

        Args:
            remove_emojis: ì´ëª¨ì§€ ì œê±° ì—¬ë¶€ (ê¸°ë³¸ê°’: False, ê°ì • ë¶„ì„ì— ìœ ìš©í•  ìˆ˜ ìˆìŒ)
            remove_special_chars: íŠ¹ìˆ˜ë¬¸ì ì œê±° ì—¬ë¶€
            normalize_repeated_chars: ë°˜ë³µ ë¬¸ì ì •ê·œí™” ì—¬ë¶€ (ã…‹ã…‹ã…‹ã…‹ â†’ ã…‹ã…‹)
            max_repeated_chars: ìµœëŒ€ í—ˆìš© ë°˜ë³µ ë¬¸ì ìˆ˜
            remove_html_entities: HTML ì—”í‹°í‹° ì œê±° ì—¬ë¶€
            normalize_whitespace: ê³µë°± ì •ê·œí™” ì—¬ë¶€
            lowercase: ì†Œë¬¸ì ë³€í™˜ ì—¬ë¶€ (í•œê¸€ì—ëŠ” ì˜í–¥ ì—†ìŒ)
        """
        self.remove_emojis = remove_emojis
        self.remove_special_chars = remove_special_chars
        self.normalize_repeated_chars = normalize_repeated_chars
        self.max_repeated_chars = max_repeated_chars
        self.remove_html_entities = remove_html_entities
        self.normalize_whitespace = normalize_whitespace
        self.lowercase = lowercase

    def clean(self, text: str) -> str:
        """
        í…ìŠ¤íŠ¸ ì •ì œ.

        Args:
            text: ì›ë³¸ í…ìŠ¤íŠ¸

        Returns:
            ì •ì œëœ í…ìŠ¤íŠ¸
        """
        if not text:
            return ""

        result = text

        # 1. HTML ì—”í‹°í‹° ì œê±°
        if self.remove_html_entities:
            result = self._remove_html_entities(result)

        # 2. Unicode ì •ê·œí™” (NFC: í•œê¸€ ìëª¨ ê²°í•©)
        result = unicodedata.normalize("NFC", result)

        # 3. ì´ëª¨ì§€ ì²˜ë¦¬
        if self.remove_emojis:
            result = self._remove_emojis(result)

        # 4. íŠ¹ìˆ˜ë¬¸ì ì œê±°
        if self.remove_special_chars:
            result = self._remove_special_chars(result)

        # 5. ë°˜ë³µ ë¬¸ì ì •ê·œí™”
        if self.normalize_repeated_chars:
            result = self._normalize_repeated_chars(result)

        # 6. ê³µë°± ì •ê·œí™”
        if self.normalize_whitespace:
            result = self._normalize_whitespace(result)

        # 7. ì†Œë¬¸ì ë³€í™˜
        if self.lowercase:
            result = result.lower()

        return result.strip()

    def _remove_html_entities(self, text: str) -> str:
        """HTML ì—”í‹°í‹° ì œê±°."""
        # ì¼ë°˜ì ì¸ HTML ì—”í‹°í‹°ë¥¼ ì‹¤ì œ ë¬¸ìë¡œ ë³€í™˜
        replacements = {
            "&nbsp;": " ",
            "&lt;": "<",
            "&gt;": ">",
            "&amp;": "&",
            "&quot;": '"',
            "&apos;": "'",
            "&#39;": "'",
        }
        for entity, char in replacements.items():
            text = text.replace(entity, char)

        # ë‚˜ë¨¸ì§€ HTML ì—”í‹°í‹° ì œê±°
        return HTML_ENTITY_PATTERN.sub("", text)

    def _remove_emojis(self, text: str) -> str:
        """ì´ëª¨ì§€ ì œê±°."""
        return EMOJI_PATTERN.sub("", text)

    def _remove_special_chars(self, text: str) -> str:
        """íŠ¹ìˆ˜ë¬¸ì ì œê±° (ì´ëª¨ì§€ ë³´ì¡´ ì˜µì…˜ ê³ ë ¤)."""
        if not self.remove_emojis:
            # ì´ëª¨ì§€ë¥¼ ë³´ì¡´í•´ì•¼ í•˜ëŠ” ê²½ìš°, ì´ëª¨ì§€ë¥¼ ì„ì‹œë¡œ ì¹˜í™˜ í›„ ë³µì›
            emoji_placeholder = {}
            counter = 0

            def save_emoji(match):
                nonlocal counter
                # ì•ŒíŒŒë²³ìœ¼ë¡œë§Œ êµ¬ì„±ëœ í”Œë ˆì´ìŠ¤í™€ë” ì‚¬ìš© (íŠ¹ìˆ˜ë¬¸ì íŒ¨í„´ì— ê±¸ë¦¬ì§€ ì•ŠìŒ)
                key = f"EMOJIPLACEHOLDER{counter}END"
                emoji_placeholder[key] = match.group(0)
                counter += 1
                return key

            text = EMOJI_PATTERN.sub(save_emoji, text)
            text = SPECIAL_CHAR_PATTERN.sub(" ", text)

            # ì´ëª¨ì§€ ë³µì›
            for key, emoji in emoji_placeholder.items():
                text = text.replace(key, emoji)
            return text
        else:
            return SPECIAL_CHAR_PATTERN.sub(" ", text)

    def _normalize_repeated_chars(self, text: str) -> str:
        """ë°˜ë³µ ë¬¸ì ì •ê·œí™”."""
        return REPEATED_CHAR_PATTERN.sub(r"\1" * self.max_repeated_chars, text)

    def _normalize_whitespace(self, text: str) -> str:
        """ê³µë°± ì •ê·œí™”."""
        # ì—°ì† ê³µë°±ì„ ë‹¨ì¼ ê³µë°±ìœ¼ë¡œ
        text = MULTIPLE_SPACES_PATTERN.sub(" ", text)
        # ì¤„ë°”ê¿ˆ ì „í›„ ê³µë°± ì œê±°
        lines = [line.strip() for line in text.split("\n")]
        return "\n".join(line for line in lines if line)


class TextChunker:
    """í…ìŠ¤íŠ¸ ì²­í‚¹ í´ë˜ìŠ¤."""

    # í•œêµ­ì–´ ë¬¸ì¥ ì¢…ê²° íŒ¨í„´
    SENTENCE_ENDINGS = re.compile(r"([.!?~]+)\s*")

    def __init__(
        self,
        chunk_size: int = 500,
        chunk_overlap: int = 50,
        min_chunk_size: int = 100,
        split_by_sentence: bool = True,
    ):
        """
        ì´ˆê¸°í™”.

        Args:
            chunk_size: ì²­í¬ ìµœëŒ€ í¬ê¸° (ë¬¸ì ìˆ˜)
            chunk_overlap: ì²­í¬ ê°„ ì˜¤ë²„ë© í¬ê¸°
            min_chunk_size: ìµœì†Œ ì²­í¬ í¬ê¸° (ì´ë³´ë‹¤ ì‘ìœ¼ë©´ ì´ì „ ì²­í¬ì— ë³‘í•©)
            split_by_sentence: ë¬¸ì¥ ë‹¨ìœ„ ë¶„í•  ì—¬ë¶€ (Trueë©´ ë¬¸ì¥ ê²½ê³„ì—ì„œ ë¶„í• )
        """
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        self.min_chunk_size = min_chunk_size
        self.split_by_sentence = split_by_sentence

    def chunk(self, text: str) -> list[TextChunk]:
        """
        í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í• .

        Args:
            text: ë¶„í• í•  í…ìŠ¤íŠ¸

        Returns:
            TextChunk ë¦¬ìŠ¤íŠ¸
        """
        if not text:
            return []

        # í…ìŠ¤íŠ¸ê°€ ì²­í¬ í¬ê¸°ë³´ë‹¤ ì‘ìœ¼ë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜
        if len(text) <= self.chunk_size:
            return [
                TextChunk(
                    text=text,
                    chunk_index=0,
                    start_char=0,
                    end_char=len(text),
                )
            ]

        if self.split_by_sentence:
            return self._chunk_by_sentence(text)
        else:
            return self._chunk_by_size(text)

    def _chunk_by_sentence(self, text: str) -> list[TextChunk]:
        """ë¬¸ì¥ ë‹¨ìœ„ë¡œ ì²­í‚¹."""
        # ë¬¸ì¥ìœ¼ë¡œ ë¶„ë¦¬
        sentences = self._split_into_sentences(text)

        chunks = []
        current_chunk = ""
        current_start = 0
        chunk_index = 0

        for sentence in sentences:
            # í˜„ì¬ ì²­í¬ì— ë¬¸ì¥ ì¶”ê°€ ì‹œ í¬ê¸° í™•ì¸
            if len(current_chunk) + len(sentence) <= self.chunk_size:
                current_chunk += sentence
            else:
                # í˜„ì¬ ì²­í¬ê°€ ìµœì†Œ í¬ê¸° ì´ìƒì´ë©´ ì €ì¥
                if len(current_chunk) >= self.min_chunk_size:
                    chunks.append(
                        TextChunk(
                            text=current_chunk.strip(),
                            chunk_index=chunk_index,
                            start_char=current_start,
                            end_char=current_start + len(current_chunk),
                        )
                    )
                    chunk_index += 1

                    # ì˜¤ë²„ë© ê³„ì‚°
                    overlap_text = current_chunk[-self.chunk_overlap :] if self.chunk_overlap > 0 else ""
                    current_start = current_start + len(current_chunk) - len(overlap_text)
                    current_chunk = overlap_text + sentence
                else:
                    # ìµœì†Œ í¬ê¸° ë¯¸ë‹¬ì´ë©´ ê³„ì† ì¶”ê°€
                    current_chunk += sentence

        # ë§ˆì§€ë§‰ ì²­í¬ ì²˜ë¦¬
        if current_chunk.strip():
            # ë§ˆì§€ë§‰ ì²­í¬ê°€ ë„ˆë¬´ ì‘ìœ¼ë©´ ì´ì „ ì²­í¬ì— ë³‘í•©
            if len(current_chunk) < self.min_chunk_size and chunks:
                last_chunk = chunks[-1]
                chunks[-1] = TextChunk(
                    text=last_chunk.text + " " + current_chunk.strip(),
                    chunk_index=last_chunk.chunk_index,
                    start_char=last_chunk.start_char,
                    end_char=current_start + len(current_chunk),
                )
            else:
                chunks.append(
                    TextChunk(
                        text=current_chunk.strip(),
                        chunk_index=chunk_index,
                        start_char=current_start,
                        end_char=current_start + len(current_chunk),
                    )
                )

        return chunks

    def _chunk_by_size(self, text: str) -> list[TextChunk]:
        """í¬ê¸° ê¸°ë°˜ ì²­í‚¹ (ë¬¸ì¥ ë¬´ì‹œ)."""
        chunks = []
        chunk_index = 0
        start = 0

        while start < len(text):
            end = min(start + self.chunk_size, len(text))

            # ë‹¨ì–´ ê²½ê³„ì—ì„œ ìë¥´ê¸° ì‹œë„
            if end < len(text):
                # ê³µë°±ì„ ì°¾ì•„ì„œ ê±°ê¸°ì„œ ìë¥´ê¸°
                last_space = text.rfind(" ", start, end)
                if last_space > start + self.min_chunk_size:
                    end = last_space

            chunk_text = text[start:end].strip()
            if chunk_text:
                chunks.append(
                    TextChunk(
                        text=chunk_text,
                        chunk_index=chunk_index,
                        start_char=start,
                        end_char=end,
                    )
                )
                chunk_index += 1

            # ë‹¤ìŒ ì‹œì‘ ìœ„ì¹˜ (ì˜¤ë²„ë© ì ìš©)
            start = end - self.chunk_overlap if end < len(text) else end

        return chunks

    def _split_into_sentences(self, text: str) -> list[str]:
        """í…ìŠ¤íŠ¸ë¥¼ ë¬¸ì¥ìœ¼ë¡œ ë¶„ë¦¬."""
        # ë¬¸ì¥ ì¢…ê²° ë¶€í˜¸ë¡œ ë¶„ë¦¬
        parts = self.SENTENCE_ENDINGS.split(text)

        sentences = []
        i = 0
        while i < len(parts):
            sentence = parts[i]
            # ì¢…ê²° ë¶€í˜¸ê°€ ìˆìœ¼ë©´ ë¶™ì´ê¸°
            if i + 1 < len(parts) and self.SENTENCE_ENDINGS.match(parts[i + 1]):
                sentence += parts[i + 1]
                i += 2
            else:
                i += 1

            if sentence.strip():
                sentences.append(sentence)

        return sentences


class DuplicateFilter:
    """ì¤‘ë³µ ë¦¬ë·° í•„í„°ë§ í´ë˜ìŠ¤."""

    def __init__(
        self,
        use_fuzzy: bool = False,
        fuzzy_threshold: float = 0.9,
    ):
        """
        ì´ˆê¸°í™”.

        Args:
            use_fuzzy: í¼ì§€ ë§¤ì¹­ ì‚¬ìš© ì—¬ë¶€ (ìœ ì‚¬ í…ìŠ¤íŠ¸ ì¤‘ë³µ ê°ì§€)
            fuzzy_threshold: í¼ì§€ ë§¤ì¹­ ì„ê³„ê°’ (0.0 ~ 1.0)
        """
        self.use_fuzzy = use_fuzzy
        self.fuzzy_threshold = fuzzy_threshold
        self._seen_hashes: set[str] = set()
        self._seen_texts: list[str] = []  # í¼ì§€ ë§¤ì¹­ìš©

    def reset(self) -> None:
        """í•„í„° ìƒíƒœ ì´ˆê¸°í™”."""
        self._seen_hashes.clear()
        self._seen_texts.clear()

    def is_duplicate(self, text: str) -> bool:
        """
        ì¤‘ë³µ ì—¬ë¶€ í™•ì¸.

        Args:
            text: í™•ì¸í•  í…ìŠ¤íŠ¸

        Returns:
            Trueë©´ ì¤‘ë³µ
        """
        text_hash = self._compute_hash(text)

        # ì •í™•íˆ ë™ì¼í•œ í…ìŠ¤íŠ¸
        if text_hash in self._seen_hashes:
            return True

        # í¼ì§€ ë§¤ì¹­
        if self.use_fuzzy:
            for seen_text in self._seen_texts:
                if self._similarity(text, seen_text) >= self.fuzzy_threshold:
                    return True

        return False

    def add(self, text: str) -> None:
        """
        í…ìŠ¤íŠ¸ë¥¼ í•„í„°ì— ì¶”ê°€.

        Args:
            text: ì¶”ê°€í•  í…ìŠ¤íŠ¸
        """
        text_hash = self._compute_hash(text)
        self._seen_hashes.add(text_hash)

        if self.use_fuzzy:
            self._seen_texts.append(text)

    def filter(self, texts: list[str]) -> list[str]:
        """
        ì¤‘ë³µ ì œê±°ëœ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜.

        Args:
            texts: í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸

        Returns:
            ì¤‘ë³µì´ ì œê±°ëœ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
        """
        result = []
        for text in texts:
            if not self.is_duplicate(text):
                self.add(text)
                result.append(text)
        return result

    @staticmethod
    def _compute_hash(text: str) -> str:
        """í…ìŠ¤íŠ¸ í•´ì‹œ ê³„ì‚°."""
        # ê³µë°± ì •ê·œí™” í›„ í•´ì‹œ
        normalized = " ".join(text.split())
        return hashlib.md5(normalized.encode("utf-8")).hexdigest()

    @staticmethod
    def _similarity(text1: str, text2: str) -> float:
        """
        ë‘ í…ìŠ¤íŠ¸ì˜ ìœ ì‚¬ë„ ê³„ì‚° (Jaccard ìœ ì‚¬ë„).

        Args:
            text1: ì²« ë²ˆì§¸ í…ìŠ¤íŠ¸
            text2: ë‘ ë²ˆì§¸ í…ìŠ¤íŠ¸

        Returns:
            ìœ ì‚¬ë„ (0.0 ~ 1.0)
        """
        # ë‹¨ì–´ ì§‘í•©ìœ¼ë¡œ ë³€í™˜
        words1 = set(text1.split())
        words2 = set(text2.split())

        if not words1 or not words2:
            return 0.0

        # Jaccard ìœ ì‚¬ë„
        intersection = len(words1 & words2)
        union = len(words1 | words2)

        return intersection / union if union > 0 else 0.0


class ReviewPreprocessor:
    """ë¦¬ë·° ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ í´ë˜ìŠ¤."""

    def __init__(
        self,
        cleaner: TextCleaner | None = None,
        chunker: TextChunker | None = None,
        duplicate_filter: DuplicateFilter | None = None,
        min_text_length: int = 10,
        max_text_length: int = 10000,
    ):
        """
        ì´ˆê¸°í™”.

        Args:
            cleaner: í…ìŠ¤íŠ¸ ì •ì œê¸° (Noneì´ë©´ ê¸°ë³¸ê°’ ì‚¬ìš©)
            chunker: í…ìŠ¤íŠ¸ ì²­ì»¤ (Noneì´ë©´ ê¸°ë³¸ê°’ ì‚¬ìš©)
            duplicate_filter: ì¤‘ë³µ í•„í„° (Noneì´ë©´ ê¸°ë³¸ê°’ ì‚¬ìš©)
            min_text_length: ìµœì†Œ í…ìŠ¤íŠ¸ ê¸¸ì´ (ì´ë³´ë‹¤ ì§§ìœ¼ë©´ í•„í„°ë§)
            max_text_length: ìµœëŒ€ í…ìŠ¤íŠ¸ ê¸¸ì´ (ì´ë³´ë‹¤ ê¸¸ë©´ ì˜ë¼ëƒ„)
        """
        self.cleaner = cleaner or TextCleaner()
        self.chunker = chunker or TextChunker()
        self.duplicate_filter = duplicate_filter or DuplicateFilter()
        self.min_text_length = min_text_length
        self.max_text_length = max_text_length

    def process(self, review: Review, skip_duplicate_check: bool = False) -> ProcessedReview | None:
        """
        ë‹¨ì¼ ë¦¬ë·° ì „ì²˜ë¦¬.

        Args:
            review: ì›ë³¸ ë¦¬ë·°
            skip_duplicate_check: ì¤‘ë³µ ê²€ì‚¬ ìŠ¤í‚µ ì—¬ë¶€

        Returns:
            ì „ì²˜ë¦¬ëœ ë¦¬ë·° (í•„í„°ë§ëœ ê²½ìš° None)
        """
        original_text = review.text

        # 1. í…ìŠ¤íŠ¸ ê¸¸ì´ ê²€ì‚¬
        if len(original_text) < self.min_text_length:
            return None

        # 2. ìµœëŒ€ ê¸¸ì´ ì œí•œ
        if len(original_text) > self.max_text_length:
            original_text = original_text[: self.max_text_length]

        # 3. í…ìŠ¤íŠ¸ ì •ì œ
        cleaned_text = self.cleaner.clean(original_text)

        # ì •ì œ í›„ ê¸¸ì´ ì¬ê²€ì‚¬
        if len(cleaned_text) < self.min_text_length:
            return None

        # 4. ì¤‘ë³µ ê²€ì‚¬
        if not skip_duplicate_check:
            if self.duplicate_filter.is_duplicate(cleaned_text):
                return None
            self.duplicate_filter.add(cleaned_text)

        # 5. ì²­í‚¹
        chunks = self.chunker.chunk(cleaned_text)

        # 6. í•´ì‹œ ê³„ì‚°
        text_hash = DuplicateFilter._compute_hash(cleaned_text)

        # 7. ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
        metadata = self._extract_metadata(review)

        return ProcessedReview(
            original_text=review.text,
            cleaned_text=cleaned_text,
            chunks=chunks,
            text_hash=text_hash,
            rating=review.rating,
            date=review.date,
            metadata=metadata,
        )

    def process_batch(
        self,
        reviews: list[Review],
        skip_duplicate_check: bool = False,
    ) -> list[ProcessedReview]:
        """
        ë¦¬ë·° ë°°ì¹˜ ì „ì²˜ë¦¬.

        Args:
            reviews: ì›ë³¸ ë¦¬ë·° ë¦¬ìŠ¤íŠ¸
            skip_duplicate_check: ì¤‘ë³µ ê²€ì‚¬ ìŠ¤í‚µ ì—¬ë¶€

        Returns:
            ì „ì²˜ë¦¬ëœ ë¦¬ë·° ë¦¬ìŠ¤íŠ¸
        """
        results = []
        for review in reviews:
            processed = self.process(review, skip_duplicate_check)
            if processed:
                results.append(processed)
        return results

    def iter_process(
        self,
        reviews: Iterator[Review],
        skip_duplicate_check: bool = False,
    ) -> Iterator[ProcessedReview]:
        """
        ë¦¬ë·° ì´í„°ë ˆì´í„° ì „ì²˜ë¦¬.

        Args:
            reviews: ì›ë³¸ ë¦¬ë·° ì´í„°ë ˆì´í„°
            skip_duplicate_check: ì¤‘ë³µ ê²€ì‚¬ ìŠ¤í‚µ ì—¬ë¶€

        Yields:
            ì „ì²˜ë¦¬ëœ ë¦¬ë·°
        """
        for review in reviews:
            processed = self.process(review, skip_duplicate_check)
            if processed:
                yield processed

    def _extract_metadata(self, review: Review) -> dict[str, Any]:
        """
        ë¦¬ë·°ì—ì„œ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ.

        Args:
            review: ì›ë³¸ ë¦¬ë·°

        Returns:
            ì¶”ì¶œëœ ë©”íƒ€ë°ì´í„°
        """
        metadata = dict(review.metadata) if review.metadata else {}

        # ê¸°ë³¸ ë©”íƒ€ë°ì´í„° ì¶”ê°€
        if review.author:
            metadata["author"] = review.author

        if review.option:
            metadata["option"] = review.option

        if review.helpful_count is not None:
            metadata["helpful_count"] = review.helpful_count

        if review.verified_purchase:
            metadata["verified_purchase"] = review.verified_purchase

        if review.images:
            metadata["has_images"] = True
            metadata["image_count"] = len(review.images)

        # í…ìŠ¤íŠ¸ í†µê³„
        metadata["text_length"] = len(review.text)
        metadata["word_count"] = len(review.text.split())

        return metadata

    def reset(self) -> None:
        """íŒŒì´í”„ë¼ì¸ ìƒíƒœ ì´ˆê¸°í™” (ì¤‘ë³µ í•„í„° ë¦¬ì…‹)."""
        self.duplicate_filter.reset()

    def get_statistics(self, processed_reviews: list[ProcessedReview]) -> dict[str, Any]:
        """
        ì „ì²˜ë¦¬ ê²°ê³¼ í†µê³„ ë°˜í™˜.

        Args:
            processed_reviews: ì „ì²˜ë¦¬ëœ ë¦¬ë·° ë¦¬ìŠ¤íŠ¸

        Returns:
            í†µê³„ ì •ë³´
        """
        if not processed_reviews:
            return {"total": 0}

        total = len(processed_reviews)
        total_chunks = sum(len(r.chunks) for r in processed_reviews)
        text_lengths = [len(r.cleaned_text) for r in processed_reviews]
        ratings = [r.rating for r in processed_reviews if r.rating is not None]

        return {
            "total_reviews": total,
            "total_chunks": total_chunks,
            "avg_chunks_per_review": total_chunks / total,
            "avg_text_length": sum(text_lengths) / total,
            "min_text_length": min(text_lengths),
            "max_text_length": max(text_lengths),
            "avg_rating": sum(ratings) / len(ratings) if ratings else None,
            "reviews_with_rating": len(ratings),
        }


def create_default_preprocessor(
    chunk_size: int = 500,
    chunk_overlap: int = 50,
    remove_emojis: bool = False,
    use_fuzzy_dedup: bool = False,
) -> ReviewPreprocessor:
    """
    ê¸°ë³¸ ì„¤ì •ì˜ ì „ì²˜ë¦¬ê¸° ìƒì„±.

    Args:
        chunk_size: ì²­í¬ í¬ê¸°
        chunk_overlap: ì²­í¬ ì˜¤ë²„ë©
        remove_emojis: ì´ëª¨ì§€ ì œê±° ì—¬ë¶€
        use_fuzzy_dedup: í¼ì§€ ì¤‘ë³µ ì œê±° ì‚¬ìš© ì—¬ë¶€

    Returns:
        ReviewPreprocessor ì¸ìŠ¤í„´ìŠ¤
    """
    cleaner = TextCleaner(
        remove_emojis=remove_emojis,
        remove_special_chars=True,
        normalize_repeated_chars=True,
    )

    chunker = TextChunker(
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap,
        split_by_sentence=True,
    )

    duplicate_filter = DuplicateFilter(
        use_fuzzy=use_fuzzy_dedup,
        fuzzy_threshold=0.9,
    )

    return ReviewPreprocessor(
        cleaner=cleaner,
        chunker=chunker,
        duplicate_filter=duplicate_filter,
    )


def main():
    """í…ŒìŠ¤íŠ¸ ì‹¤í–‰."""
    from pathlib import Path

    from src.pipeline.aihub_loader import AIHubDataLoader

    # AI Hub ë°ì´í„° ë¡œë“œ
    data_dir = Path(__file__).parent.parent.parent / "data" / "aihub_data"

    if not data_dir.exists():
        print(f"ë°ì´í„° ë””ë ‰í† ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤: {data_dir}")
        print("ìƒ˜í”Œ ë°ì´í„°ë¡œ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤.")

        # ìƒ˜í”Œ ë¦¬ë·°ë¡œ í…ŒìŠ¤íŠ¸
        sample_reviews = [
            Review(
                text="ì´ ì œí’ˆ ì •ë§ ì¢‹ì•„ìš”ã…‹ã…‹ã…‹ã…‹ã…‹ ë°°ì†¡ë„ ë¹¨ë¼ì„œ ë§Œì¡±í•©ë‹ˆë‹¤!!! ğŸ‘ğŸ‘",
                rating=5.0,
                date="2024-01-15",
            ),
            Review(
                text="í’ˆì§ˆì´ ê¸°ëŒ€ì— ëª» ë¯¸ì¹˜ë„¤ìš”... ê°€ê²© ëŒ€ë¹„ ë³„ë¡œì…ë‹ˆë‹¤.",
                rating=2.0,
                date="2024-01-14",
            ),
            Review(
                text="ì´ ì œí’ˆ ì •ë§ ì¢‹ì•„ìš” ë°°ì†¡ë„ ë¹¨ë¼ì„œ ë§Œì¡±í•©ë‹ˆë‹¤!",  # ìœ ì‚¬ ì¤‘ë³µ
                rating=5.0,
                date="2024-01-13",
            ),
        ]
    else:
        loader = AIHubDataLoader(data_dir)
        sample_reviews = loader.load_reviews(limit=10, as_project_format=True)

    # ì „ì²˜ë¦¬ê¸° ìƒì„±
    preprocessor = create_default_preprocessor(
        chunk_size=300,
        chunk_overlap=30,
        remove_emojis=True,
        use_fuzzy_dedup=True,
    )

    # ì „ì²˜ë¦¬ ì‹¤í–‰
    print("=== ì „ì²˜ë¦¬ í…ŒìŠ¤íŠ¸ ===\n")

    processed = preprocessor.process_batch(sample_reviews)

    print(f"ì›ë³¸ ë¦¬ë·° ìˆ˜: {len(sample_reviews)}")
    print(f"ì „ì²˜ë¦¬ í›„ ë¦¬ë·° ìˆ˜: {len(processed)}")

    if processed:
        print(f"\n=== ì „ì²˜ë¦¬ ê²°ê³¼ ìƒ˜í”Œ ===\n")
        for i, p in enumerate(processed[:3], 1):
            print(f"[{i}] ì›ë³¸: {p.original_text[:50]}...")
            print(f"    ì •ì œ: {p.cleaned_text[:50]}...")
            print(f"    ì²­í¬ ìˆ˜: {len(p.chunks)}")
            print(f"    í‰ì : {p.rating}")
            print(f"    í•´ì‹œ: {p.text_hash[:16]}...")
            print()

        # í†µê³„ ì¶œë ¥
        stats = preprocessor.get_statistics(processed)
        print("=== ì „ì²˜ë¦¬ í†µê³„ ===")
        for key, value in stats.items():
            if isinstance(value, float):
                print(f"  {key}: {value:.2f}")
            else:
                print(f"  {key}: {value}")


if __name__ == "__main__":
    main()
