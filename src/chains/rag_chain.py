"""
RAG (Retrieval-Augmented Generation) Chain êµ¬í˜„.

ë¦¬ë·° ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ” RAG ì²´ì¸ì„ ì œê³µí•©ë‹ˆë‹¤.
"""

import os
from dataclasses import dataclass, field
from typing import Any, AsyncIterator, Iterator

from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain.schema import Document
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain_openai import ChatOpenAI

from src.pipeline.embedder import ReviewEmbedder, create_embedder
from src.prompts.templates import QA_PROMPT, SUMMARY_PROMPT, get_prompt, PromptTemplate


# ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ (prompts ëª¨ë“ˆì—ì„œ ê°€ì ¸ì˜´)
SYSTEM_PROMPT = QA_PROMPT.system_prompt
USER_PROMPT_TEMPLATE = QA_PROMPT.user_prompt_template


@dataclass
class RAGResponse:
    """RAG ì‘ë‹µ ë°ì´í„° êµ¬ì¡°."""

    answer: str
    source_documents: list[Document] = field(default_factory=list)
    metadata: dict[str, Any] = field(default_factory=dict)


@dataclass
class RAGConfig:
    """RAG Chain ì„¤ì •."""

    # LLM ì„¤ì •
    model_name: str = "gpt-4o-mini"
    temperature: float = 0.0
    max_tokens: int = 2048
    streaming: bool = True

    # ê²€ìƒ‰ ì„¤ì •
    top_k: int = 5
    search_type: str = "similarity"  # "similarity" or "mmr"

    # í”„ë¡¬í”„íŠ¸ ì„¤ì •
    system_prompt: str = SYSTEM_PROMPT
    user_prompt_template: str = USER_PROMPT_TEMPLATE


class ReviewRAGChain:
    """ë¦¬ë·° RAG Chain í´ë˜ìŠ¤."""

    def __init__(
        self,
        embedder: ReviewEmbedder | None = None,
        config: RAGConfig | None = None,
        openai_api_key: str | None = None,
    ):
        """
        ì´ˆê¸°í™”.

        Args:
            embedder: ReviewEmbedder ì¸ìŠ¤í„´ìŠ¤ (Noneì´ë©´ ìƒˆë¡œ ìƒì„±)
            config: RAG ì„¤ì • (Noneì´ë©´ ê¸°ë³¸ê°’ ì‚¬ìš©)
            openai_api_key: OpenAI API í‚¤
        """
        self.config = config or RAGConfig()

        # API í‚¤ ì„¤ì •
        self._api_key = openai_api_key or os.getenv("OPENAI_API_KEY")
        if not self._api_key:
            raise ValueError(
                "OpenAI API í‚¤ê°€ í•„ìš”í•©ë‹ˆë‹¤. "
                "OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ë¥¼ ì„¤ì •í•˜ê±°ë‚˜ openai_api_key íŒŒë¼ë¯¸í„°ë¥¼ ì „ë‹¬í•˜ì„¸ìš”."
            )

        # Embedder ì„¤ì •
        self.embedder = embedder or create_embedder(openai_api_key=self._api_key)

        # LLM ì´ˆê¸°í™”
        self._llm = ChatOpenAI(
            model=self.config.model_name,
            temperature=self.config.temperature,
            max_tokens=self.config.max_tokens,
            streaming=self.config.streaming,
            openai_api_key=self._api_key,
        )

        # Retriever ì„¤ì •
        self._retriever = self.embedder.get_retriever(
            search_type=self.config.search_type,
            top_k=self.config.top_k,
        )

        # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ìƒì„±
        self._prompt = ChatPromptTemplate.from_messages([
            ("system", self.config.system_prompt),
            ("human", self.config.user_prompt_template),
        ])

        # RAG Chain êµ¬ì„±
        self._chain = self._build_chain()

    def _build_chain(self):
        """RAG Chain êµ¬ì„±."""

        def format_docs(docs: list[Document]) -> str:
            """ë¬¸ì„œ í¬ë§·íŒ…."""
            formatted = []
            for i, doc in enumerate(docs, 1):
                rating = doc.metadata.get("rating", "N/A")
                date = doc.metadata.get("date", "N/A")
                text = doc.page_content

                formatted.append(
                    f"[ë¦¬ë·° {i}] (í‰ì : {rating}, ë‚ ì§œ: {date})\n{text}"
                )

            return "\n\n".join(formatted)

        # Chain êµ¬ì„±: ê²€ìƒ‰ â†’ í¬ë§·íŒ… â†’ í”„ë¡¬í”„íŠ¸ â†’ LLM â†’ íŒŒì‹±
        chain = (
            {
                "context": self._retriever | format_docs,
                "question": RunnablePassthrough(),
            }
            | self._prompt
            | self._llm
            | StrOutputParser()
        )

        return chain

    def query(self, question: str) -> RAGResponse:
        """
        ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ ìƒì„±.

        Args:
            question: ì‚¬ìš©ì ì§ˆë¬¸

        Returns:
            RAGResponse ê°ì²´
        """
        # ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰
        source_docs = self._retriever.invoke(question)

        # ë‹µë³€ ìƒì„±
        answer = self._chain.invoke(question)

        return RAGResponse(
            answer=answer,
            source_documents=source_docs,
            metadata={
                "model": self.config.model_name,
                "top_k": self.config.top_k,
                "num_sources": len(source_docs),
            },
        )

    def stream(self, question: str) -> Iterator[str]:
        """
        ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ìœ¼ë¡œ ë‹µë³€ ìƒì„±.

        Args:
            question: ì‚¬ìš©ì ì§ˆë¬¸

        Yields:
            ë‹µë³€ í…ìŠ¤íŠ¸ ì²­í¬
        """
        for chunk in self._chain.stream(question):
            yield chunk

    async def astream(self, question: str) -> AsyncIterator[str]:
        """
        ë¹„ë™ê¸° ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ìœ¼ë¡œ ë‹µë³€ ìƒì„±.

        Args:
            question: ì‚¬ìš©ì ì§ˆë¬¸

        Yields:
            ë‹µë³€ í…ìŠ¤íŠ¸ ì²­í¬
        """
        async for chunk in self._chain.astream(question):
            yield chunk

    def query_with_sources(self, question: str) -> dict[str, Any]:
        """
        ì¶œì²˜ì™€ í•¨ê»˜ ë‹µë³€ ë°˜í™˜.

        Args:
            question: ì‚¬ìš©ì ì§ˆë¬¸

        Returns:
            ë‹µë³€ê³¼ ì¶œì²˜ ì •ë³´ë¥¼ í¬í•¨í•œ ë”•ì…”ë„ˆë¦¬
        """
        response = self.query(question)

        sources = []
        for doc in response.source_documents:
            # original_textê°€ ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ ì²­í¬ í…ìŠ¤íŠ¸ ì‚¬ìš©
            text = doc.metadata.get("original_text") or doc.page_content
            sources.append({
                "text": text,
                "rating": doc.metadata.get("rating"),
                "date": doc.metadata.get("date"),
                "review_hash": doc.metadata.get("review_hash"),
            })

        return {
            "answer": response.answer,
            "sources": sources,
            "metadata": response.metadata,
        }

    def update_config(self, **kwargs) -> None:
        """
        ì„¤ì • ì—…ë°ì´íŠ¸.

        Args:
            **kwargs: ì—…ë°ì´íŠ¸í•  ì„¤ì • ê°’ë“¤
        """
        for key, value in kwargs.items():
            if hasattr(self.config, key):
                setattr(self.config, key, value)

        # LLM ì¬ì´ˆê¸°í™”
        if any(k in kwargs for k in ["model_name", "temperature", "max_tokens", "streaming"]):
            self._llm = ChatOpenAI(
                model=self.config.model_name,
                temperature=self.config.temperature,
                max_tokens=self.config.max_tokens,
                streaming=self.config.streaming,
                openai_api_key=self._api_key,
            )

        # Retriever ì¬ì„¤ì •
        if any(k in kwargs for k in ["top_k", "search_type"]):
            self._retriever = self.embedder.get_retriever(
                search_type=self.config.search_type,
                top_k=self.config.top_k,
            )

        # Chain ì¬êµ¬ì„±
        self._chain = self._build_chain()

    def set_prompt(self, prompt_name: str) -> None:
        """
        í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ë³€ê²½.

        Args:
            prompt_name: í”„ë¡¬í”„íŠ¸ ì´ë¦„ ("qa", "summary", "compare", "sentiment")
        """
        prompt = get_prompt(prompt_name)
        self.config.system_prompt = prompt.system_prompt
        self.config.user_prompt_template = prompt.user_prompt_template

        # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì¬ìƒì„±
        self._prompt = ChatPromptTemplate.from_messages([
            ("system", self.config.system_prompt),
            ("human", self.config.user_prompt_template),
        ])

        # Chain ì¬êµ¬ì„±
        self._chain = self._build_chain()

    def set_prompt_template(self, prompt_template: PromptTemplate) -> None:
        """
        ì»¤ìŠ¤í…€ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì„¤ì •.

        Args:
            prompt_template: PromptTemplate ì¸ìŠ¤í„´ìŠ¤
        """
        self.config.system_prompt = prompt_template.system_prompt
        self.config.user_prompt_template = prompt_template.user_prompt_template

        # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì¬ìƒì„±
        self._prompt = ChatPromptTemplate.from_messages([
            ("system", self.config.system_prompt),
            ("human", self.config.user_prompt_template),
        ])

        # Chain ì¬êµ¬ì„±
        self._chain = self._build_chain()

    @property
    def retriever(self):
        """Retriever ë°˜í™˜."""
        return self._retriever

    @property
    def llm(self):
        """LLM ë°˜í™˜."""
        return self._llm


def create_rag_chain(
    embedder: ReviewEmbedder | None = None,
    model_name: str = "gpt-4o-mini",
    temperature: float = 0.0,
    top_k: int = 5,
    streaming: bool = True,
    openai_api_key: str | None = None,
) -> ReviewRAGChain:
    """
    RAG Chain ìƒì„± í—¬í¼ í•¨ìˆ˜.

    Args:
        embedder: ReviewEmbedder ì¸ìŠ¤í„´ìŠ¤
        model_name: LLM ëª¨ë¸ëª…
        temperature: ì˜¨ë„ ì„¤ì •
        top_k: ê²€ìƒ‰í•  ë¬¸ì„œ ìˆ˜
        streaming: ìŠ¤íŠ¸ë¦¬ë° ì‚¬ìš© ì—¬ë¶€
        openai_api_key: OpenAI API í‚¤

    Returns:
        ReviewRAGChain ì¸ìŠ¤í„´ìŠ¤
    """
    config = RAGConfig(
        model_name=model_name,
        temperature=temperature,
        top_k=top_k,
        streaming=streaming,
    )

    return ReviewRAGChain(
        embedder=embedder,
        config=config,
        openai_api_key=openai_api_key,
    )


def main():
    """í…ŒìŠ¤íŠ¸ ì‹¤í–‰."""
    from dotenv import load_dotenv

    from src.crawler.base import Review
    from src.pipeline.preprocessor import create_default_preprocessor

    # í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
    load_dotenv()

    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        print("OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return

    print("=" * 60)
    print("ğŸ¤– RAG Chain í…ŒìŠ¤íŠ¸")
    print("=" * 60)

    # 1. ìƒ˜í”Œ ë¦¬ë·° ë°ì´í„°
    sample_reviews = [
        Review(
            text="ì´ ì œí’ˆ ì •ë§ ì¢‹ì•„ìš”! ë°°ì†¡ë„ ë¹ ë¥´ê³  í’ˆì§ˆë„ í›Œë¥­í•©ë‹ˆë‹¤. ê°€ê²© ëŒ€ë¹„ ë§Œì¡±ìŠ¤ëŸ½ìŠµë‹ˆë‹¤.",
            rating=5.0,
            date="2024-01-15",
        ),
        Review(
            text="ë°°ì†¡ì€ ë¹¨ëëŠ”ë° ì œí’ˆ í’ˆì§ˆì´ ê¸°ëŒ€ì— ëª» ë¯¸ì¹˜ë„¤ìš”. ê°€ê²©ì´ ì¢€ ì•„ê¹ìŠµë‹ˆë‹¤.",
            rating=2.0,
            date="2024-01-14",
        ),
        Review(
            text="ë¬´ë‚œí•œ ì œí’ˆì…ë‹ˆë‹¤. íŠ¹ë³„íˆ ì¢‹ì§€ë„ ë‚˜ì˜ì§€ë„ ì•Šì•„ìš”. ê·¸ëƒ¥ í‰ë²”í•©ë‹ˆë‹¤.",
            rating=3.0,
            date="2024-01-13",
        ),
        Review(
            text="ë°°ì†¡ì´ ì •ë§ ë¹¨ë¼ì„œ ë†€ëì–´ìš”! ì£¼ë¬¸ ë‹¤ìŒë‚  ë„ì°©í–ˆìŠµë‹ˆë‹¤. ì œí’ˆë„ ê´œì°®ë„¤ìš”.",
            rating=4.0,
            date="2024-01-12",
        ),
        Review(
            text="ì‚¬ì´ì¦ˆê°€ ìƒê°ë³´ë‹¤ ì‘ì•„ìš”. êµí™˜í•˜ë ¤ë‹ˆ ë°°ì†¡ë¹„ê°€ ì•„ê¹Œì›Œì„œ ê·¸ëƒ¥ ì”ë‹ˆë‹¤.",
            rating=2.5,
            date="2024-01-11",
        ),
    ]

    # 2. ì „ì²˜ë¦¬
    print("\nğŸ“ 1ë‹¨ê³„: ë¦¬ë·° ì „ì²˜ë¦¬ ë° ì„ë² ë”©")
    preprocessor = create_default_preprocessor(chunk_size=300)
    processed_reviews = preprocessor.process_batch(sample_reviews)
    print(f"   â†’ {len(processed_reviews)}ê°œ ë¦¬ë·° ì „ì²˜ë¦¬ ì™„ë£Œ")

    # 3. Embedder ìƒì„± ë° ë°ì´í„° ì¶”ê°€
    embedder = create_embedder(
        collection_name="rag_test_reviews",
        persist_directory="./data/chroma_db_test",
    )
    embedder.reset_collection()
    embedder.add_reviews(processed_reviews)
    print(f"   â†’ ë²¡í„° DBì— ì €ì¥ ì™„ë£Œ")

    # 4. RAG Chain ìƒì„±
    print("\nğŸ”— 2ë‹¨ê³„: RAG Chain ì´ˆê¸°í™”")
    rag_chain = create_rag_chain(
        embedder=embedder,
        model_name="gpt-4o-mini",
        top_k=3,
    )
    print("   â†’ RAG Chain ìƒì„± ì™„ë£Œ")

    # 5. ì§ˆì˜ì‘ë‹µ í…ŒìŠ¤íŠ¸
    test_questions = [
        "ë°°ì†¡ì´ ë¹ ë¥¸ê°€ìš”?",
        "í’ˆì§ˆì€ ì–´ë–¤ê°€ìš”?",
        "ê°€ê²© ëŒ€ë¹„ ê°€ì¹˜ê°€ ìˆë‚˜ìš”?",
        "ì‚¬ì´ì¦ˆëŠ” ì–´ë–¤ê°€ìš”?",
    ]

    print("\nğŸ’¬ 3ë‹¨ê³„: ì§ˆì˜ì‘ë‹µ í…ŒìŠ¤íŠ¸")
    for question in test_questions:
        print(f"\n{'â”€' * 50}")
        print(f"â“ ì§ˆë¬¸: {question}")
        print(f"{'â”€' * 50}")

        result = rag_chain.query_with_sources(question)

        print(f"\nğŸ“ ë‹µë³€:\n{result['answer']}")

        print(f"\nğŸ“š ì°¸ì¡° ë¦¬ë·° ({len(result['sources'])}ê°œ):")
        for i, source in enumerate(result['sources'], 1):
            rating = source.get('rating', 'N/A')
            print(f"   [{i}] (í‰ì : {rating}) {source['text'][:50]}...")

    # 6. ìŠ¤íŠ¸ë¦¬ë° í…ŒìŠ¤íŠ¸
    print(f"\n{'â”€' * 50}")
    print("ğŸŒŠ ìŠ¤íŠ¸ë¦¬ë° í…ŒìŠ¤íŠ¸")
    print(f"{'â”€' * 50}")
    print("\nâ“ ì§ˆë¬¸: ì´ ì œí’ˆì„ ì¶”ì²œí•˜ì‹œë‚˜ìš”?")
    print("\nğŸ“ ë‹µë³€: ", end="", flush=True)

    for chunk in rag_chain.stream("ì´ ì œí’ˆì„ ì¶”ì²œí•˜ì‹œë‚˜ìš”?"):
        print(chunk, end="", flush=True)
    print()

    # 7. ì •ë¦¬
    print("\nğŸ§¹ í…ŒìŠ¤íŠ¸ ë°ì´í„° ì •ë¦¬")
    embedder.delete_collection()
    print("   â†’ í…ŒìŠ¤íŠ¸ ì»¬ë ‰ì…˜ ì‚­ì œ ì™„ë£Œ")

    print("\n" + "=" * 60)
    print("âœ… RAG Chain í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
    print("=" * 60)


if __name__ == "__main__":
    main()
